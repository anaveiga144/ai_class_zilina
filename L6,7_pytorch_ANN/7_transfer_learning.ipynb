{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkj4rzVsmjut",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "Ah37fuaGrlms",
    "outputId": "14ea1dfb-7773-44bf-b3bf-8afd91a45209"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7-4YhzosUs8"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision import datasets, models, transforms\n",
    "from skorch import NeuralNetClassifier, NeuralNetRegressor\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping\n",
    "from skorch.helper import predefined_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uxPlOyi3sUo_",
    "outputId": "3470537d-92c1-44fa-cd9d-a557210aa437"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "!mkdir -p data/food5v2\n",
    "!wget -nc -O data/food5v2.zip https://www.dropbox.com/s/w4pg809npvatye0/food5v2.zip?dl=1\n",
    "!unzip -oq -d data/food5v2 data/food5v2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eX4E0ZOHIpSH"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "def predefined_array_split(X_valid, Y_valid):\n",
    "    return predefined_split(\n",
    "        TensorDataset(\n",
    "            torch.as_tensor(X_valid),\n",
    "            torch.as_tensor(Y_valid)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2trJ0VSKmjyK",
    "tags": [
     "en"
    ]
   },
   "source": [
    "As usual, we will select the device to run the network on automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yu8JAZrEtsze"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bm8Dnrk5OC-l",
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "In this notebook we will use the **Food 5** dataset to illustrate transfer learning. The dataset is a downsized version of the [Food 11](https://www.kaggle.com/vermaavi/food11) dataset.\n",
    "\n",
    "Transfer learning is a very useful technique. Under ordinary circumstance deep learning requires a huge amount of data and computation. If we intend to apply it to a small dataset we will typically not be able to achieve good generalization. The problem is connected to the fact that a small dataset typically cannot sufficiently cover all the possible variations of samples that a model can encounter. In the case of image recognition, for instance, there is virtually an infinite number of variations that a photo of a dog can take: the environment, the lighting, the breed of the dog, the angle â€“ these and other aspects can all change. A small dataset is very unlikely to cover such complex space sufficiently.\n",
    "\n",
    "One of the solutions that allow us to apply deep learning to small datasets even in spite of these problems is **transfer learning**. Under this technique the neural network is first pre-trained on a large, more general dataset (for image recognition this tends to be the ImageNet dataset). The network uses this dataset to learn what natural images look like and how they need to be preprocessed. Once this pre-training is complete, the dataset is then further trained for the specific target task.\n",
    "\n",
    "## The Overall Procedure\n",
    "\n",
    "The overall procedure for transfer learning in image recognition:\n",
    "* Pre-train a network on ImageNet.\n",
    "\n",
    "* Remove one or several of the final layers (the top of the network) and replace them with new layers. The new output layer will now have as many outputs as there are classes in the dataset.\n",
    "\n",
    "* The weights of the pre-trained layers are frozen. Only the new layers are trained using the target dataset.\n",
    "\n",
    "* One the new layers have been trained we can (an optional step) unfreeze the weights of the pre-trained layers as well and fine-tune the network as a whole. We will need to use a significantly lower learning rate. This is so that we do not destroy the pre-trained layers by doing excessively aggressive updates, but also because when the pre-trained layers can be modified, the risk of overfitting tends to increase.\n",
    "\n",
    "## Preparation of the Dataset\n",
    "\n",
    "As usual, let us start by preparing our dataset. For most image recognition tasks the dataset will be too large to fit into memory at once. We will therefore typically not attempt to load all the data at once as ``numpy`` arrays the way we did up till now. We will instead use the dataset abstraction from ``PyTorch``: we will construct an object that will represent our data and that will load the images from the hard drive upon request.\n",
    "\n",
    "In the present case, our data comes pre-split into the train, validation and test folds, with each stored in a separate folder. The folders are structured so that each class has its own subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p-Wiw8QHmjy5",
    "outputId": "fcf06264-909b-4fb3-bf1c-7d3d3c3767ac"
   },
   "outputs": [],
   "source": [
    "!ls data/food5v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vt3NIEDKmjzC",
    "outputId": "20c55ccf-e265-44b6-c4d4-34e93b008511"
   },
   "outputs": [],
   "source": [
    "!ls data/food5v2/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVCdEtsFmjzP",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Given that our data has this structure, we can use the ``ImageFolder`` dataset class from ``PyTorch``.\n",
    "\n",
    "Every image will need to be preprocessed before it is passed to the neural network: it will need to be resized, cropped and normalized in a way that matches the preprocessing that was done when pre-training the original neural network. We will now define two preprocessing procedures. The first one will do standard preprocessing. The second one will include augmentation: there will be a few randomized steps that will modify the image every time that it is loaded. This is to add more variety to the training data: essentially, the network will never see the exact same image twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6P8XQEMCsgVX"
   },
   "outputs": [],
   "source": [
    "normal_preproc = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "augment_preproc = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndwMqTUOR71l",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Next we can construct the ``ImageFolder`` datasets themselves. We specify the paths to the individual folds of our dataset as well as the way in which the images should be preprocessed for each fold. We will use the normal pipeline for validation and testing data and the pipeline with augmentation for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEMPcRIits1V"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    \"data/food5v2/training\",\n",
    "    augment_preproc\n",
    ")\n",
    "\n",
    "valid_dataset = datasets.ImageFolder(\n",
    "    \"data/food5v2/validation\",\n",
    "    normal_preproc\n",
    ")\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    \"data/food5v2/testing\",\n",
    "    normal_preproc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwehgtZCmjz5",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Displaying a Few Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "QgmXBE6mmj0B",
    "outputId": "3481ef95-15b0-4276-c922-21681a9fb2f7"
   },
   "outputs": [],
   "source": [
    "#@title -- Display Data Samples --\n",
    "disp_dataset = datasets.ImageFolder(\n",
    "    \"data/food5v2/training\",\n",
    "    transforms.ToTensor()\n",
    ")\n",
    "loader = DataLoader(disp_dataset, batch_size=1, shuffle=True)\n",
    "loader_iter = iter(loader)\n",
    "\n",
    "num_rows = 4; num_cols = 4\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 8))\n",
    "\n",
    "for row in axes:\n",
    "    for ax in row:\n",
    "        sample = next(loader_iter)[0][0].numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(sample)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aLHWrpgOJOz",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Loading the Pre-Trained Network\n",
    "\n",
    "We load a pre-trained ResNet50 network. The weights pre-trained on ImageNet will download automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "62b5fd4a93c24ed79e48653a9a9a25ee",
      "4cd6f7bc5c35493d8df30c8d8c50d341",
      "afcf29de38cb41fc8864c97ee3d9868f",
      "9dd061a14394421189614ae7a81445bb",
      "648f58a22d394427aed5eb0fe2cc1fd8",
      "37d77ec765bf4a90b23f74b92193d40f",
      "aab8a6aeb1d044afa95a6f767b875dce",
      "3ba50fd181e54b4892b46993d2a9287a"
     ]
    },
    "colab_type": "code",
    "id": "iSHPQsGVmj0v",
    "outputId": "4364cff6-5048-42a4-db38-9e945f8fab49"
   },
   "outputs": [],
   "source": [
    "net = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PpgGwXhmj05",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Replacing the Final Layer\n",
    "\n",
    "We will replace the last, fully-connected layer of our resnet (``net.fc``) with a new module that contains a dropout layer, a fully-connected layer and the softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tmi9KFLfBIqZ"
   },
   "outputs": [],
   "source": [
    "class ModelTop(nn.Module):\n",
    "    def __init__(self, num_features, num_outputs):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(num_features, num_outputs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = torch.flatten(x, 1)\n",
    "        y = self.dropout(y)\n",
    "        y = self.fc(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lut1-SsStUA"
   },
   "outputs": [],
   "source": [
    "top = ModelTop(num_features=net.fc.in_features, num_outputs=10)\n",
    "net.fc = top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ovrd5B8mj1V",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Training the New Layers\n",
    "\n",
    "Recall that at first, we only want to train our new top layers and leave the pre-trained layers as they are. We will therefore need to freeze all the layers except last by flipping the ``requires_grad`` flag for all their parameters to ``False``. We define an auxiliary function that does this and call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92v6X8Dcmj1c"
   },
   "outputs": [],
   "source": [
    "def freeze_except_last(model, freeze):\n",
    "    for layer in list(model.children())[:-1]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = not freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ey8iP-hWY8bi"
   },
   "outputs": [],
   "source": [
    "freeze_except_last(net, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZytOULSKNzAR",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We create our usual ``NeuralNetClassifier`` object. We will be using a ``Checkpoint`` callback. This will ensure that the best (lowest validation loss) version of the weights will always get saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRQQ8yJ1O6XK"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = NeuralNetClassifier(\n",
    "    net,\n",
    "    max_epochs=20,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    "    callbacks=[Checkpoint(f_params=\"train.pt\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "U1RNT4bVO6Pn",
    "outputId": "5fe4d4b0-754a-4e27-a34b-d985bc66719b"
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset, y=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5wQxDmnlNzAo",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Once the training is finished, we load the weights from the checkpoint file. This will ensure that we continue on with the best weights found during training rather than the last weights (which might possibly have overfitted already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wN8vUW6ZtgoF"
   },
   "outputs": [],
   "source": [
    "model.load_params(f_params=\"train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYnxsjbxNzA2",
    "tags": [
     "en"
    ]
   },
   "source": [
    "After this first phase we are ready for testing. However, we are not done with our model yet so we will only be testing it on the **validation set, not on the testing set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZjHi0dfyuIaD",
    "outputId": "79c63053-bf38-4cd9-ea0b-52b164d3d601"
   },
   "outputs": [],
   "source": [
    "Y_valid = []\n",
    "y_valid = []\n",
    "\n",
    "for X_batch, Y_batch in model.get_iterator(valid_dataset):\n",
    "    Y_valid.extend(Y_batch.numpy())\n",
    "    y_valid.extend(model.predict(X_batch))\n",
    "\n",
    "print(\"Validation set accuracy: {}.\".format(\n",
    "    accuracy_score(Y_valid, y_valid)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUbFpxSmmj2K",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Fine-tuning the Entire Model\n",
    "\n",
    "Having trained the new top of the model, we will now unfreeze all the rest of the network and continue training. However, we will lower the learning rate significantly to ensure that we do not undo all the work by taking overly aggressive steps.\n",
    "\n",
    "This time we will also be using early stopping so we add it as a further callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EacmsKfoQxkd"
   },
   "outputs": [],
   "source": [
    "freeze_except_last(net, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umiDrHJGQxvV"
   },
   "outputs": [],
   "source": [
    "model = NeuralNetClassifier(\n",
    "    net,\n",
    "    max_epochs=40,\n",
    "    batch_size=64,\n",
    "    lr=1e-5,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=predefined_split(valid_dataset),\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device, \n",
    "    callbacks=[\n",
    "        Checkpoint(f_params=\"finetune.pt\"),\n",
    "        EarlyStopping(patience=15)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "50SA_tUYQxsT",
    "outputId": "9bef0c41-86c5-43a5-9617-5a77e9411d89"
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset, y=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFCGENhKNzB5",
    "tags": [
     "en"
    ]
   },
   "source": [
    "After training, we will again restore the best weights using the checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1iU-r8bCoo-w"
   },
   "outputs": [],
   "source": [
    "model.load_params(f_params=\"finetune.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYqSHvT_PExc",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Testing\n",
    "\n",
    "Having trained the final version of our model, we will now test on all 3 folds of our data: the training, the validation and the testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ehMq6hVGsBPf",
    "outputId": "442803d8-3efb-4d80-a30d-95fe33a1ad03"
   },
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "y_train = []\n",
    "\n",
    "for X_batch, Y_batch in model.get_iterator(train_dataset):\n",
    "    Y_train.extend(Y_batch.numpy())\n",
    "    y_train.extend(model.predict(X_batch))\n",
    "    \n",
    "print(\"Train set accuracy: {}.\".format(\n",
    "    accuracy_score(Y_train, y_train)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VnQAKE0AJQLD",
    "outputId": "d5b84b3d-21dc-4d24-94d6-b762b985cc06"
   },
   "outputs": [],
   "source": [
    "Y_valid = []\n",
    "y_valid = []\n",
    "\n",
    "for X_batch, Y_batch in model.get_iterator(valid_dataset):\n",
    "    Y_valid.extend(Y_batch.numpy())\n",
    "    y_valid.extend(model.predict(X_batch))\n",
    "    \n",
    "print(\"Validation set accuracy: {}.\".format(\n",
    "    accuracy_score(Y_valid, y_valid)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ri3BORbPJQW1",
    "outputId": "451aaf04-bf02-4fa5-a9c1-05332aaad627"
   },
   "outputs": [],
   "source": [
    "Y_test = []\n",
    "y_test = []\n",
    "\n",
    "for X_batch, Y_batch in model.get_iterator(test_dataset):\n",
    "    Y_test.extend(Y_batch.numpy())\n",
    "    y_test.extend(model.predict(X_batch))\n",
    "    \n",
    "print(\"Test set accuracy: {}.\".format(\n",
    "    accuracy_score(Y_test, y_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUl7oL7Imj3E",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## An Alternative: Using the Pre-trained Network as a Feature Extractor\n",
    "\n",
    "There is an alternative approach that we can take: we can use the pre-trained network as a feature extractor and use it to pre-process the dataset. We would then train the new top as a separate network, which would be significantly faster because the preprocessing would already have been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIOgMaFyUY5o"
   },
   "outputs": [],
   "source": [
    "pretrained_net = models.resnet50(pretrained=True)\n",
    "pretrained_net.fc = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eqvc4e-kU41y"
   },
   "outputs": [],
   "source": [
    "feature_extractor = NeuralNetRegressor(\n",
    "    pretrained_net,\n",
    "    batch_size=64,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "feature_extractor.initialize();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flKUlhLxGoym"
   },
   "outputs": [],
   "source": [
    "def preproc_data(feature_extractor, dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for X_batch, Y_batch in feature_extractor.get_iterator(dataset):\n",
    "        X.extend(feature_extractor.predict(X_batch))\n",
    "        Y.extend(Y_batch.numpy())\n",
    "  \n",
    "    return np.asarray(X), np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjuWfNhhWA8q"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = preproc_data(feature_extractor, train_dataset)\n",
    "X_valid, Y_valid = preproc_data(feature_extractor, valid_dataset)\n",
    "X_test, Y_test = preproc_data(feature_extractor, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJVrHdx0HIor"
   },
   "outputs": [],
   "source": [
    "top_net = ModelTop(X_train.shape[1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJ3R4EifHIjU"
   },
   "outputs": [],
   "source": [
    "top_model = NeuralNetClassifier(\n",
    "    top_net,\n",
    "    max_epochs=500,\n",
    "    batch_size=200,\n",
    "    lr=1e-3,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=predefined_array_split(X_valid, Y_valid),\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device,\n",
    "    callbacks=[\n",
    "        Checkpoint(f_params=\"train_top.pt\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5GpnnSvkHq2Y",
    "outputId": "e55bcfcb-174f-4e24-c788-a3a0c3efa07b"
   },
   "outputs": [],
   "source": [
    "top_model.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uL4s75brIDV6"
   },
   "outputs": [],
   "source": [
    "top_model.load_params(f_params=\"train_top.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXj780fpQITB",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tIXGZ_S3KiQN",
    "outputId": "edad535e-b950-4764-8656-abfa127a1d16"
   },
   "outputs": [],
   "source": [
    "y_valid = top_model.predict(X_valid)\n",
    "\n",
    "print(\"Validation set accuracy: {}.\".format(\n",
    "    accuracy_score(Y_valid, y_valid)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jei9NZymJIii",
    "outputId": "cfab94cf-65aa-4331-a38e-69706c92baac"
   },
   "outputs": [],
   "source": [
    "y_test = top_model.predict(X_test)\n",
    "\n",
    "print(\"Test set accuracy: {}.\".format(\n",
    "    accuracy_score(Y_test, y_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QiJ5ur2v9xnz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "7_transfer_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "37d77ec765bf4a90b23f74b92193d40f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ba50fd181e54b4892b46993d2a9287a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cd6f7bc5c35493d8df30c8d8c50d341": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62b5fd4a93c24ed79e48653a9a9a25ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_afcf29de38cb41fc8864c97ee3d9868f",
       "IPY_MODEL_9dd061a14394421189614ae7a81445bb"
      ],
      "layout": "IPY_MODEL_4cd6f7bc5c35493d8df30c8d8c50d341"
     }
    },
    "648f58a22d394427aed5eb0fe2cc1fd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9dd061a14394421189614ae7a81445bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ba50fd181e54b4892b46993d2a9287a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_aab8a6aeb1d044afa95a6f767b875dce",
      "value": "100% 97.8M/97.8M [00:00&lt;00:00, 196MB/s]"
     }
    },
    "aab8a6aeb1d044afa95a6f767b875dce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "afcf29de38cb41fc8864c97ee3d9868f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37d77ec765bf4a90b23f74b92193d40f",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_648f58a22d394427aed5eb0fe2cc1fd8",
      "value": 102502400
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
