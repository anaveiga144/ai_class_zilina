{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gbvd2Q1MnLGG",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "Bt9xyBAWyzUP",
    "outputId": "1b2ff352-ec3c-4a21-89fd-ff6e8cd047ef"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "z2-NXRAuFhl6",
    "outputId": "36b4b5dd-476a-452d-8517-ee96a5f5f9c7"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNN3tkqMnLG5",
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Classifying MNIST Digits\n",
    "\n",
    "This example will illustrate how to construct a simple convolutional network for image classification on the MNIST handwritten digits dataset.\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "We will start by loading the MNIST dataset. This step will be very simple – all we need to do is to use function ``fetch_openml`` from ``scikit-learn``, which will download it for us. The dataset comes pre-split into the train and test sets: to get this standard split, we will use the first 60000 samples for training and the remaining 10000 samples for testing.\n",
    "\n",
    "At the same time we will cast the data into the appropriate data types and ensure that the arrays have correct shapes. Our data is composed of $28 \\times 28$ pixel images with a single colour channel. In ``PyTorch`` colour channels are represented by the zeroth dimension of the array, which is why are reshaping into (1, 28, 28).\n",
    "\n",
    "When converting the types, we also divide the data by 255 so as to transform from integers between 0 and 255 to floats between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54gu9mO0vZT6"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoaaEB3NxvQx"
   },
   "outputs": [],
   "source": [
    "X = mnist.data.reshape((-1, 1, 28, 28)).astype('float32') / 255\n",
    "Y = mnist.target.astype('int64')\n",
    "X_train = X[:60000]\n",
    "Y_train = Y[:60000]\n",
    "X_test = X[:60000]\n",
    "Y_test = Y[:60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txWaLDB_nLHh",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We can now display a few randomly selected examples from the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "0ZmVUulDGXH-",
    "outputId": "f4096cbf-745c-4677-be0c-d31bb56f9517"
   },
   "outputs": [],
   "source": [
    "num_rows = 4; num_cols = 4\n",
    "fig, axes = plt.subplots(num_rows, num_cols)\n",
    "\n",
    "for row in axes:\n",
    "    for ax in row:\n",
    "        ax.imshow(X_train[np.random.randint(0,\n",
    "                            len(X_train)-1), 0],\n",
    "                  cmap='Greys')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GphfkazlnLHy",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Constructing the Convolutional Network\n",
    "\n",
    "When constructing a convolutional net the procedure is usually to study literature that deals with similar tasks and use that knowledge to design a similar neural architecture for the problem at hand (and possibly to tune it).\n",
    "\n",
    "Given that the MNIST dataset is not especially difficult, we will use to illustrate an even simpler approach:\n",
    "* We will keep chaining blocks of convolutional layers, ReLU functions and pooling layers.\n",
    "* We will keep going until the dimensions of the inputs have decreased sufficiently.\n",
    "* Once that happens we will append one or several standard linear layers and ReLUs.\n",
    "* Finally, we will append the output layer with the softmax activation function.\n",
    "\n",
    "To make it easier to keep track of what the dimensions of the output are after all the individual layers have been applied, we will not wrap our layers into a class just yet: we will instead experiment with them freely first. To this end we will take a few samples from the dataset, which will be used as a dummy input. We need to cast these into a tensor before we feed them into ``PyTorch``: we will use the ``torch.as_tensor`` function to do this. So far we were able to avoid the step because the ``skorch`` interface took care of it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0VLWcWQ_o4d"
   },
   "outputs": [],
   "source": [
    "y = torch.as_tensor(X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tnUChCYtXBc",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let us now create our first block and apply it to tensor ``y``. We will first create our 2D convolutional layer using class ``nn.Conv2d``. We need to specify a few parameters: namely the number of input and output channels and the kernel size. The number of input channels is 1, of course, because as we mentioned, we have a single colour channel. The number of output channels is a hyperparameter – we are going to use 32.\n",
    "\n",
    "Convolutional kernels can be of different sizes, but the conventional wisdom based on empirical evidence is that $3 \\times 3$ kernels tend to work well. Making the kernel unnecessarily large is something we want to avoid because the larger the matrices we are working with, the longer it will take to multiply them.\n",
    "\n",
    "After the convolutional layer we apply the ReLU activation function and max-pooling, for which we again need to specify a kernel size. With pooling, the larger the kernel size, the more rapidly our data will be downsampled. We are therefore using a small $2 \\times 2$ kernel. A number of modern architectures have now dispensed with the use of pooling layers altogether and use strides or dilations in the convolutional layer to downsample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3aFchVN5lJD"
   },
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(\n",
    "    in_channels=1, out_channels=32,\n",
    "    kernel_size=(3, 3))\n",
    "\n",
    "y = conv1(y)\n",
    "y = torch.relu(y)\n",
    "y = torch.max_pool2d(y, kernel_size=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMKEQ8zQnLIS",
    "tags": [
     "en"
    ]
   },
   "source": [
    "After we have constructed our first block, let us check what effect this had on the dimensionality of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M7URiPUI5lTK",
    "outputId": "9a434e5e-aacb-4ef4-f7f1-0ba353dafc21"
   },
   "outputs": [],
   "source": [
    "np.product(y.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uibE1TnAnLIl",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Alas, our data still has too many dimensions and we need to reduce its dimensionality further. Let's try to apply one more block to it and let's also use a somewhat lower number of output channels now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnRZLp7h5lmV"
   },
   "outputs": [],
   "source": [
    "conv2 = nn.Conv2d(32, 16, (3, 3))\n",
    "y = conv2(y)\n",
    "y = torch.relu(y)\n",
    "y = torch.max_pool2d(y, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Gt8KrJsC5liQ",
    "outputId": "b58606c1-bfce-4b1e-ea09-0c18e1812180"
   },
   "outputs": [],
   "source": [
    "np.product(y.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arpQnD5nnLI9",
    "tags": [
     "en"
    ]
   },
   "source": [
    "The number of dimensions is much more reasonable now. We can now flatten the output (transform it from a 2-dimensional image into a 1-dimensional vector) and apply some standard linear layers and ReLUs. Again we make sure that the dimension of the data decreases gradually and the change from one layer to the next is not too drastic. The output layer is going to have 10 output neurons, because we are going to classify into 10 classes: the digits. We will use softmax as its activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFx8XKrA5lei"
   },
   "outputs": [],
   "source": [
    "y = torch.flatten(y, 1)\n",
    "\n",
    "fc1 = nn.Linear(400, 128)\n",
    "y = fc1(y)\n",
    "y = torch.relu(y)\n",
    "\n",
    "fc2 = nn.Linear(128, 10)\n",
    "y = fc2(y)\n",
    "y = torch.softmax(y, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qrBx4t6nLJM",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Now that we have designed our architecture, we need to wrap it in a class again. As usual, layers with parameters need to be constructed in ``__init__`` (in our case this is true of all layers from ``nn.`` but not for layers from ``torch.``) and then reused whenever ``forward`` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPiXuQRQ5lFk"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_outputs = 10\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, (3, 3))\n",
    "        self.conv2 = nn.Conv2d(32, 16, (3, 3))\n",
    "        self.fc1 = nn.Linear(400, 128)\n",
    "        self.fc2 = nn.Linear(128, num_outputs)\n",
    "\n",
    "    def forward(self, y):\n",
    "        y = self.conv1(y)\n",
    "        y = torch.relu(y)\n",
    "        y = torch.max_pool2d(y, kernel_size=(2, 2))\n",
    "        \n",
    "        y = self.conv2(y)\n",
    "        y = torch.relu(y)\n",
    "        y = torch.max_pool2d(y, kernel_size=(2, 2))\n",
    "        \n",
    "        y = torch.flatten(y, 1)\n",
    "        \n",
    "        y = self.fc1(y)\n",
    "        y = torch.relu(y)\n",
    "\n",
    "        y = self.fc2(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbhYh26qn0NA",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Constructing and Training the Classifier\n",
    "\n",
    "The construction of a ``NeuralNetClassifier`` and training will be analogical to what we did in our previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rOPs2eHBkpG"
   },
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=5,\n",
    "    batch_size=128,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=None,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "_0XpfQP4Bkl-",
    "outputId": "ee33947a-030b-4847-977e-c729889c7506"
   },
   "outputs": [],
   "source": [
    "net.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9GtNTcBBn3Z_",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Testing\n",
    "\n",
    "Finally, we apply our standard testing procedure for classifiers: we display the confusion matrix and the accuracy on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdNPPQojC2ai"
   },
   "outputs": [],
   "source": [
    "y_test = net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "3e0nhLl4C2V2",
    "outputId": "d06c5702-abe7-48c7-ca4d-7e49576c9a33"
   },
   "outputs": [],
   "source": [
    "cm = pd.crosstab(Y_test, y_test,\n",
    "    rownames=['actual'],\n",
    "    colnames=['predicted']\n",
    ")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MKyKQaemDCDy",
    "outputId": "8b2329f6-b36a-4b40-c33f-25abd1ddc229"
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(Y_test, y_test)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAGmqsvFVOPR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "6_cnn_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
