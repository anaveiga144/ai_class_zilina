{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S-z0oogD8gL",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "rhIBCV6AD4qB",
    "outputId": "6da7ca52-ff69-406f-fb7b-4451180dc560"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "mi0rP21wD4kf",
    "outputId": "1554573f-ff4d-4753-99ab-8fe015e20681"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from skorch import NeuralNetClassifier, NeuralNet\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "ynFqI0klD4gU",
    "outputId": "1e431c0f-1e80-40da-a00a-90310e53222f"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "!mkdir -p data\n",
    "!wget -nv -nc -O data/lion.png https://www.dropbox.com/s/djnjkz456tbgfnk/lion.png?dl=1\n",
    "!wget -nv -nc -O data/imagenet_classes https://www.dropbox.com/s/ma25i7w3jpqex2a/imagenet_classes?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7J1si6ND4Yz"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with open(\"data/imagenet_classes\", \"r\") as file:\n",
    "    classes = [c[:-1] for c in file.readlines()]\n",
    "    \n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "unnormalize = transforms.Normalize(\n",
    "    mean=[-tm/sm for tm, sm in zip(normalize.mean, normalize.std)],\n",
    "    std=[1.0/ts for ts in normalize.std]\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def preproc_image(img, device=device):\n",
    "    img = resize(img[:, :, :3], (224, 224))\n",
    "    img_t = transform(img)\n",
    "    batch_t = torch.unsqueeze(img_t, 0)\n",
    "    return batch_t.to(device)\n",
    "\n",
    "def deproc_image(img_prep):\n",
    "    img = unnormalize(\n",
    "        img_prep[0].to('cpu')\n",
    "    ).detach().numpy().transpose((1, 2, 0))\n",
    "    return np.minimum(np.maximum(img, 0.0), 1.0)\n",
    "\n",
    "def decode_proba(proba, top=5):\n",
    "    proba = proba.ravel()\n",
    "    ind = np.argsort(proba)\n",
    "    \n",
    "    for c in reversed(ind[-top:]):\n",
    "        print(\"{}:\\t{} ({})\".format(\n",
    "            np.array2string(proba[c], precision=5,\n",
    "                            suppress_small=False),\n",
    "            classes[c], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zepJcAkZg41q",
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Adversarial Examples\n",
    "\n",
    "This notebook shows one relatively simple method for generating adversarial examples.\n",
    "\n",
    "Let us start by loading the 50-layer ResNet architecture pretrained on ImageNet. The network expects 224x224 images at its input and it is able to classify them into 1000 classes (their list is in file data/classes and will also be displayed in the code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "ab3bb3b09ebb45638340c089356d580b",
      "3440e18938784b088dc6287e8c47c347",
      "b6044518fa4a43bfa547626b7b46da89",
      "55febaf5bcff4777a2cb33d0695d24fd",
      "2870b73bf4fa4425a569cc9f14d3b3bb",
      "6a776623f18b414899e5d25d1a4dfbb2",
      "116cdbb34dac4a5c859ae1924715daf1",
      "27c400a84d074c8f9c3b42bd465def91"
     ]
    },
    "colab_type": "code",
    "id": "ESjg0cKOYmgu",
    "outputId": "2e3904fe-68c7-4de4-a188-4cbb52309f34"
   },
   "outputs": [],
   "source": [
    "module = models.resnet50(pretrained=True)\n",
    "num_classes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJOwvMLbhIpD",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We wrap the ResNet in our usual ``skorch`` wrapper for easy inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T07KMNIAD4NZ"
   },
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    torch.nn.Sequential(\n",
    "        module,\n",
    "        torch.nn.Softmax(dim=-1)\n",
    "    ),\n",
    "    device=device,\n",
    ")\n",
    "net.initialize();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmOKRCAphTkV",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Parameters\n",
    "\n",
    "We select the target class here: i.e. the class that we will try to get our image to be misclassifed into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSpNgmDehX73"
   },
   "outputs": [],
   "source": [
    "# target_class = 231 # collie\n",
    "# target_class = 413 # assault rifle\n",
    "# target_class = 847 # tank\n",
    "target_class = 409 # analog clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLlf_JnbiJ3w",
    "tags": [
     "en"
    ]
   },
   "source": [
    "To get the list of all the classes uncomment and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIjDOu9dhYBd"
   },
   "outputs": [],
   "source": [
    "# for ic, c in enumerate(classes):\n",
    "#     print(\"{}:\\t{}\".format(ic, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-BgUXNQi3rL",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Loading and Preprocessing the Original Image\n",
    "\n",
    "Next we are going to load and display the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "xyArDh9fD36P",
    "outputId": "08ffab95-2c5c-4fa1-9249-aefd1fd28357"
   },
   "outputs": [],
   "source": [
    "img = plt.imread(\"data/lion.png\")\n",
    "plt.imshow(img); plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_i_aIDJFjVA0",
    "tags": [
     "en"
    ]
   },
   "source": [
    " We will apply the preprocessing that our pretrained neural net expects using function ``preproc_image``. We will then run the preprocessed image through the net and display the top-5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "4NqE0MtSDYQQ",
    "outputId": "6bb6fb86-43fe-42a7-d1d6-efd11bde4d80"
   },
   "outputs": [],
   "source": [
    "img_t = preproc_image(img)\n",
    "proba = net.predict_proba(img_t)\n",
    "decode_proba(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04aMSI4Mjl1G",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Constructing the Loss Function\n",
    "\n",
    "Our next step will be to construct the loss function that we are going to minimize in order to get our adversarial image. Since it is the adversarial image that we are going to be optimizing, let us create a separate tensor for it. Given that the image is supposed to look like the original image, the sensible thing, of course, is to initialize it by copying the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w8A6FP_hhF6r"
   },
   "outputs": [],
   "source": [
    "adv_t = img_t.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9D677ndMkbx0",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Having created our adversarial tensor, we will also wrap the target class in a tensor (of type ``long``) and make sure it is transferred to the correct device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0st-aMZOhGBe"
   },
   "outputs": [],
   "source": [
    "target_class_t = torch.as_tensor([target_class], dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOp5Iir5lCod",
    "tags": [
     "en"
    ]
   },
   "source": [
    "When computing the loss, we:\n",
    "* Run the adversarial example through the network to compute its output ``y``;\n",
    "* We want the input to be misclassified into ``target_class_t`` so we construct the deception loss as the cross entropy loss with ``y`` and ``target_class_t`` as parameters (let us recall that we also use cross entropy when training a network to predict certain classes);\n",
    "* We construct the similarity loss as the $L^1$ distance between the adversarial image and the original image;\n",
    "* We add the two losses up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLbMN7vNEws9"
   },
   "outputs": [],
   "source": [
    "def compute_loss():\n",
    "    y = module(adv_t)\n",
    "    deception_loss = torch.nn.functional.cross_entropy(y, target_class_t)\n",
    "    similarity_loss = torch.nn.functional.l1_loss(adv_t, img_t)\n",
    "    loss = deception_loss + similarity_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93aDZc-emC2e",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## The Optimization\n",
    "\n",
    "We will create an optimizer and provide it with the parameters that it is going to be optimizing: tensor ``adv_t`` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2g3JbgsZmwJ5"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS([adv_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixIwQt0Omy9E",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We define a function that the optimizer is going to run at each step:\n",
    "* Zero out the gradients from the previous step.\n",
    "* Compute the loss function.\n",
    "* Backpropagate the gradients.\n",
    "\n",
    "The updating of the parameters is, of course, going to be handled by the optimizer itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nyTg-cozgL9y"
   },
   "outputs": [],
   "source": [
    "def opti_step():\n",
    "    optimizer.zero_grad()\n",
    "    loss = compute_loss()\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dsH0bn6meIK",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We run the optimizer for a couple of epochs and display the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "R4CRsaCo48b8",
    "outputId": "ed02d83e-0795-4e97-f0a8-b20422a3ae32"
   },
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    optimizer.step(opti_step)\n",
    "    print(\"Epoch {}; loss {}.\".format(epoch, compute_loss().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgeNJcsKnFri",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Displaying the Adversarial Example\n",
    "\n",
    "We will process the resulting adversarial example to transform it from a tensor back to a natural image that can be visualized. We also run the adversarial image through our network to make sure that it really does get misclassified. If everything worked out correctly, the image should now get classified as an analog clock or whatever other target class that we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "i1At6wb648YS",
    "outputId": "219cc77c-56ab-4d50-df49-f048a6f85e26"
   },
   "outputs": [],
   "source": [
    "adv = deproc_image(adv_t)\n",
    "proba = net.predict_proba(preproc_image(adv))\n",
    "decode_proba(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXi9dXhcn4bt",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We can now plot both: the original image and the adversarial image side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "pNFUSSA-nTbW",
    "outputId": "2bdbf97b-975c-4e0d-b0ad-e8950e9209fb"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[10, 6])\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(\"the original image\")\n",
    "\n",
    "axes[1].imshow(adv)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(\"the adversarial example\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omNq7w_Ln1m9",
    "tags": [
     "en"
    ]
   },
   "source": [
    "The images are going to be visually indistinguishable. To show that they are really not the same and how they differ, we will compute and display their absolute pixel-wise difference (averaging over the colour channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "SX2VWgtx48Qd",
    "outputId": "e632f133-9d26-402a-cdcb-e6ad341056d0"
   },
   "outputs": [],
   "source": [
    "diff = np.abs(img - adv).mean(axis=-1)\n",
    "plt.imshow(diff, cmap='Greys')\n",
    "plt.axis('off')\n",
    "plt.colorbar(label=\"pixel-wise difference (range [0, 1])\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_heFhuoZN2iH",
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: A Different Image and Target Class\n",
    "\n",
    "**Apply the same procedure to a different image and target class.**\n",
    "\n",
    "Note: New images can be uploaded **directly through the notebook interface** or alternatively using:\n",
    "```python\n",
    "from google.colab import files\n",
    "content_img = files.upload()\n",
    "filename = list(content_img)[0]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nqCKWHGN2iU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "8_adversarial_examples.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "116cdbb34dac4a5c859ae1924715daf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27c400a84d074c8f9c3b42bd465def91": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2870b73bf4fa4425a569cc9f14d3b3bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3440e18938784b088dc6287e8c47c347": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55febaf5bcff4777a2cb33d0695d24fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27c400a84d074c8f9c3b42bd465def91",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_116cdbb34dac4a5c859ae1924715daf1",
      "value": "100% 97.8M/97.8M [00:00&lt;00:00, 232MB/s]"
     }
    },
    "6a776623f18b414899e5d25d1a4dfbb2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab3bb3b09ebb45638340c089356d580b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6044518fa4a43bfa547626b7b46da89",
       "IPY_MODEL_55febaf5bcff4777a2cb33d0695d24fd"
      ],
      "layout": "IPY_MODEL_3440e18938784b088dc6287e8c47c347"
     }
    },
    "b6044518fa4a43bfa547626b7b46da89": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a776623f18b414899e5d25d1a4dfbb2",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2870b73bf4fa4425a569cc9f14d3b3bb",
      "value": 102502400
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
