{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7h-gdSUXtrq",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.**\n",
    "\n",
    "The notebook uses the flashtorch package and we also use the image of the toucan from the same repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "vt9sKp4g5vQC",
    "outputId": "9977e458-e09f-4715-d1ff-5f2d247c7c52"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install flashtorch skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "3E0t0wPS5vRa",
    "outputId": "a2b060ba-7e49-41ca-9b77-0ce93d24c2c5"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from flashtorch.saliency import Backprop\n",
    "from skorch import NeuralNetClassifier, NeuralNet\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "KdwWHj8V5vQo",
    "outputId": "37788031-d589-4d58-b5a1-12d3150ed5e7"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "!mkdir -p data\n",
    "!wget -nv -nc -O data/imagenet_classes https://www.dropbox.com/s/ma25i7w3jpqex2a/imagenet_classes?dl=1\n",
    "!wget -nv -nc -O data/toucan.jpg https://github.com/MisaOgura/flashtorch/raw/master/examples/images/toucan.jpg\n",
    "!wget -nv -nc -O data/car.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/DNK_ambulance_new_design.jpg/640px-DNK_ambulance_new_design.jpg\n",
    "!wget -nv -nc -O data/raccoon_example.png https://www.dropbox.com/s/qca8pk3x7ouvoo3/raccoon_example.jpg?dl=1\n",
    "!wget -nv -nc -O data/nails.jpg https://www.dropbox.com/s/at02ej24yd7h1qc/nails.jpg?dl=1\n",
    "!wget -nv -nc -O data/screws.jpg https://www.dropbox.com/s/1rp38ie35xih0dn/screws.jpg?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aU2ZTD0Q5vR6"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "with open(\"data/imagenet_classes\", \"r\") as file:\n",
    "    classes = [c[:-1] for c in file.readlines()]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "def preproc_image(img, size=224):\n",
    "    if not isinstance(img, Image.Image):\n",
    "        img = F.to_pil_image(img)\n",
    "\n",
    "    means = [0.485, 0.456, 0.406]\n",
    "    stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])\n",
    "\n",
    "    tensor = transform(img).unsqueeze(0)\n",
    "    tensor.requires_grad = True\n",
    "    return tensor\n",
    "\n",
    "def decode_proba(proba, top=5):\n",
    "    proba = proba.ravel()\n",
    "    ind = np.argsort(proba)\n",
    "    \n",
    "    for c in reversed(ind[-top:]):\n",
    "        print(\"{}:\\t{} ({})\".format(\n",
    "            np.array2string(proba[c], precision=5),\n",
    "            classes[c], c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEc0N6iSYh2E",
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Visual Interpretation of Neural Networks\n",
    "\n",
    "Neural nets with deep learning represent a powerful machine learning paradigm. However, they are not known for their interpretability. Nevertheless, there are a few techniques that can provide some insight into whether a neural net is doing what it should or not. For tabular data, predictions can of course be explained with methods such as LIME. But there is also a couple of good techniques that work for images and we are going to showcase one of them in this notebook.\n",
    "\n",
    "The approach itself is simple. We use the backprop to compute the sensitivity of the prediction to the various input pixels and visualize the result, which is sometimes referred to as a saliency map. The saliency map, roughly speaking, highlights the pixels in proportion to their relevance to the prediction. This allows us to inspect whether the network is paying attention to the correct portions of the image: i.e. that it actually predicts label \"plane\" because it recognizes the plane and not because most of the pixels in the background are blue.\n",
    "\n",
    "## Loading the Model\n",
    "\n",
    "We will start by loading a pretrained model that we are going to be testing. We also wrap it in the skorch interface to make the prediction of labels easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "e84d89d8aee74b08a282478e9944a3ef",
      "1106d4b1f464444dabb2848508432981",
      "5ac02555f3e24b25bd006cb12bccbd0b",
      "d94574de5207440a863136696f75c98d",
      "02c31ace20c848848c6c30a917c6df50",
      "e91d669d3d5b437aafcda0771710af54",
      "c0be0a5c51ff490d852d6e98b5e5f9b0",
      "e11c8826e32142f2a211747c84cdce78"
     ]
    },
    "colab_type": "code",
    "id": "VP0l6CIW5vTB",
    "outputId": "6a25ef87-9633-40e0-bedc-ef74df0bfefe"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "device = \"cuda\" if use_gpu else \"cpu\"\n",
    "model = models.densenet161(pretrained=True)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    torch.nn.Sequential(\n",
    "        model,\n",
    "        torch.nn.Softmax(dim=-1)\n",
    "    ),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "net.initialize();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n5upN6iiXtt9",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We create the ``Backprop`` explainer for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2IHGiRF25vTT"
   },
   "outputs": [],
   "source": [
    "backprop = Backprop(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3mdC2dm0XtuQ",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We load a few images, run them through our model and display the saliency maps. Note how the maps highlight the salient pixels: the images of the toucan and the ambulance are especially interesting. For the toucan, the beak is highlighted, which makes sense: that is what a human would pay attention to when trying to distinguish it from other birds. Similarly for the ambulance â€“ most attention is on the patterns and the lights.\n",
    "\n",
    "When recognizing screws, the network seems to be focusing onto the spiral pattern and when recognizing nails, the heads seem to be important as well as the body of the nail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPGgcGl65vTy"
   },
   "outputs": [],
   "source": [
    "img = Image.open('data/toucan.jpg')\n",
    "img_preproc = preproc_image(img)\n",
    "backprop.visualize(img_preproc, None, guided=True, use_gpu=use_gpu)\n",
    "decode_proba(net.predict_proba(img_preproc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "B3zG9wA25vUC",
    "outputId": "66d824f7-920b-4d8a-9861-3d8cb75912f3"
   },
   "outputs": [],
   "source": [
    "img = Image.open('data/car.jpg')\n",
    "img_preproc = preproc_image(img)\n",
    "backprop.visualize(img_preproc, None, guided=True, use_gpu=use_gpu)\n",
    "decode_proba(net.predict_proba(img_preproc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "7mGTbYZG5vUT",
    "outputId": "39729326-ae6b-4344-fd83-91a15b981693"
   },
   "outputs": [],
   "source": [
    "img = Image.open('data/screws.jpg')\n",
    "img_preproc = preproc_image(img)\n",
    "backprop.visualize(img_preproc, None, guided=True, use_gpu=use_gpu)\n",
    "decode_proba(net.predict_proba(img_preproc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "dcrzpTYL5vUk",
    "outputId": "93c9b5e1-be59-434b-f865-9077b1a78584"
   },
   "outputs": [],
   "source": [
    "img = Image.open('data/nails.jpg')\n",
    "img_preproc = preproc_image(img)\n",
    "backprop.visualize(img_preproc, None, guided=True, use_gpu=use_gpu)\n",
    "decode_proba(net.predict_proba(img_preproc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "OKMkageL5vTj",
    "outputId": "3af95cab-7157-4c85-b4bd-82f28bd35dcc"
   },
   "outputs": [],
   "source": [
    "img = Image.open('data/raccoon_example.png')\n",
    "img_preproc = preproc_image(img)\n",
    "backprop.visualize(img_preproc, None, guided=True, use_gpu=use_gpu)\n",
    "decode_proba(net.predict_proba(img_preproc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fRUti0DYpU1",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Other Visualization Techniques\n",
    "\n",
    "Naturally, there are many other visualization techniques. To get some inspiration, you may explore the [Lucid repository](https://github.com/tensorflow/lucid) from Google, which showcases some cool things: including visualizations of preimages of filters from intermediate layers and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlpHbNGNdKr9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "9_visual_interpretation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02c31ace20c848848c6c30a917c6df50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1106d4b1f464444dabb2848508432981": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ac02555f3e24b25bd006cb12bccbd0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e91d669d3d5b437aafcda0771710af54",
      "max": 115730790,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02c31ace20c848848c6c30a917c6df50",
      "value": 115730790
     }
    },
    "c0be0a5c51ff490d852d6e98b5e5f9b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d94574de5207440a863136696f75c98d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e11c8826e32142f2a211747c84cdce78",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c0be0a5c51ff490d852d6e98b5e5f9b0",
      "value": "100% 110M/110M [00:02&lt;00:00, 55.9MB/s]"
     }
    },
    "e11c8826e32142f2a211747c84cdce78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e84d89d8aee74b08a282478e9944a3ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5ac02555f3e24b25bd006cb12bccbd0b",
       "IPY_MODEL_d94574de5207440a863136696f75c98d"
      ],
      "layout": "IPY_MODEL_1106d4b1f464444dabb2848508432981"
     }
    },
    "e91d669d3d5b437aafcda0771710af54": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
