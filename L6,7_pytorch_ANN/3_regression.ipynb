{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WC94NpgjL_Ou",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PQYmhASVL_PI",
    "outputId": "76321f51-16c1-47dd-a612-27c8dc5a8883"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install skorch\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "r8y8qp8gL_Pp",
    "outputId": "74fd0918-b57f-4001-c048-2ebb40c39f78"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skorch import NeuralNetRegressor\n",
    "from class_utils import error_histogram\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qWIal29oL_P6",
    "outputId": "cbc86f04-bcfb-4249-9db5-20f6874c86d9"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "!mkdir -p data\n",
    "!mkdir -p output\n",
    "!wget -nc -O data/sigmoid_regression_data.csv https://www.dropbox.com/s/p5q7gzupa2ndw55/sigmoid_regression_data.csv?dl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQSbH9iwL_QK",
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Neural Network Based Regression\n",
    "\n",
    "In this notebook we are going to show how a simple neural net created using ``skorch`` and ``PyTorch`` can be applied to a regression problem. The code will, to a large extent, be identical to that for classification. The principal differences will consist in the activation function of the output layer (which will now be linear) and the loss function which we will be minimizing (the mean squared error).\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We will start by defining our regression problem. To this end we will first load the dataset from a CSV file – the data consists of noisy samples from a sigmoid curve. Given that we have encounter such data several times already, we will not go over the entire procedure again and the source code of the following cell is hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "WyM9YplXL_QU",
    "outputId": "7f563ca3-afae-4711-8416-8a528f37b9ef"
   },
   "outputs": [],
   "source": [
    "#@title -- Data Loading and Preprocessing; X_train, Y_train, X_test, Y_test -- { display-mode: \"form\" }\n",
    "df = pd.read_csv(\"data/sigmoid_regression_data.csv\")\n",
    "\n",
    "# we create a discretized version of the y column\n",
    "# to allow for stratification\n",
    "kbins = KBinsDiscretizer(6, encode='ordinal')\n",
    "y_stratify = kbins.fit_transform(df[['y']])\n",
    "\n",
    "# we split the dataset into train and test\n",
    "df_train, df_test = train_test_split(df, stratify=y_stratify,\n",
    "                                 test_size=0.3, random_state=4)\n",
    "\n",
    "# we specify the inputs and the outputs\n",
    "categorical_inputs = []\n",
    "numeric_inputs = ['x']\n",
    "output = ['y']\n",
    "\n",
    "# we create the pipeline\n",
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy='constant', fill_value='MISSING'),\n",
    "        OneHotEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")\n",
    "\n",
    "# we fit and apply the pipeline on the train set\n",
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs])\n",
    "Y_train = df_train[output].values\n",
    "\n",
    "# we apply the same pipeline to the test set,\n",
    "# taking care to use transform and not fit_transform\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs])\n",
    "Y_test = df_test[output].values\n",
    "\n",
    "# we plot the data for visual inspection\n",
    "plt.scatter(X_train, Y_train, marker='x', label=\"training data\")\n",
    "plt.scatter(X_test, Y_test, c='r', label=\"testing data\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "plt.savefig(\"output/regression_data.pdf\", bbox_inches='tight', ppad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OjEM-vSdL_Qj",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We transform to data types expected by PyTorch. In the case of regression 32-bit floats are expect both at the input and the output of the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D034I7pzL_Qq"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "Y_train = Y_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "Y_test = Y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GC4NtVMbL_Q2",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Creation of the Neural Network and Training\n",
    "\n",
    "Our neural network will be created in a way very similar to the neural classifier from previous examples. The main difference is that the **output layer is going to be linear** (we are not going to apply to softmax function) – this is to allow our regressor to produce unbounded outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oANQWkmvL_Q-"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = Y_train.shape[1]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y = torch.relu(y)\n",
    "        \n",
    "        y = self.fc2(y)\n",
    "        y = torch.relu(y)\n",
    "        \n",
    "        y = self.fc3(y)        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g5AwhOhQL_RI",
    "tags": [
     "en"
    ]
   },
   "source": [
    "As our wrapper class we will now use ``NeuralNetRegressor``, which has a different default loss function – with classification we were using cross entropy, while with regression we will be using the mean squared error. The arguments are more or less the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NapzXy1LL_RO"
   },
   "outputs": [],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    Net,\n",
    "    max_epochs=200,\n",
    "    batch_size=-1,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    train_split=None,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-c3fPHJzL_RZ",
    "outputId": "a3c98057-5cc7-425d-d73f-83c973a96dd4"
   },
   "outputs": [],
   "source": [
    "net.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vrjov_H4L_Rm",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Testing\n",
    "\n",
    "Finally, we will test our model on the testing set and display the regression curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "wHsXWjVIL_Rs",
    "outputId": "1480a217-2698-4766-9dbe-c8b906000c15"
   },
   "outputs": [],
   "source": [
    "#@title -- Testing -- { display-mode: \"form\" }\n",
    "y_test = net.predict(X_test)\n",
    "\n",
    "# we compute and display the MSE and the MAE\n",
    "mse = mean_squared_error(Y_test, y_test)\n",
    "print(\"MSE = {}\".format(mse))\n",
    "\n",
    "mae = mean_absolute_error(Y_test, y_test)\n",
    "print(\"MAE = {}\".format(mae))\n",
    "\n",
    "# we display the error histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "error_histogram(Y_test, y_test, Y_fit_scaling=Y_train)\n",
    "plt.savefig(\"output/error_output_histogram.pdf\", bbox_inches='tight', ppad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "UyT19LaJL_R3",
    "outputId": "5deec8cf-4ac0-4520-c276-b8f0ddff1b28"
   },
   "outputs": [],
   "source": [
    "#@title -- Regression Curve vs. Data -- { display-mode: \"form\" }\n",
    "x_min = min(np.min(X_train), np.min(X_test))\n",
    "x_max = max(np.max(X_train), np.max(X_test))\n",
    "\n",
    "xx = np.linspace(x_min, x_max, 250).reshape((-1, 1)).astype(np.float32)\n",
    "yy = net.predict(xx)\n",
    "\n",
    "plt.scatter(X_train, Y_train, marker='x', label=\"training data\")\n",
    "plt.scatter(X_test, Y_test, c='r', label=\"testing data\")\n",
    "\n",
    "plt.plot(xx, yy, label=\"regression curve\", c='k')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"output/regression.pdf\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFb8rM1eL_SB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "3_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
