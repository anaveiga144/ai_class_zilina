{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdQvsHZK37gI",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "mS_NjpD99UXq",
    "outputId": "b40c5400-ff00-4862-9454-1794451c668a"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "!git clone https://github.com/Puzer/stylegan-encoder.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "hpR06Ca637gj",
    "outputId": "d21630f5-f859-4f15-d73a-705b9cd5876b"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "%tensorflow_version 1.x\n",
    "import sys\n",
    "sys.path.append('stylegan-encoder')\n",
    "\n",
    "import os\n",
    "import bz2\n",
    "import dlib\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from keras.utils import get_file\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "\n",
    "import pickle\n",
    "import config\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "colab_type": "code",
    "id": "cHiOyi0jQX5U",
    "outputId": "0131577c-b88b-4706-a8c0-220dd87094ea"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "!mkdir -p data\n",
    "!wget -nc -O data/starr.jpg https://www.dropbox.com/s/oyr35cz55lry5my/starr.jpg?dl=1\n",
    "!wget -nc -O data/model.pkl https://www.dropbox.com/s/3rxfuwwcia8hxhj/karras2019stylegan-ffhq-1024x1024.pkl?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vq54jWwqABsN"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "LANDMARKS_MODEL_URL = \"https://www.dropbox.com/s/ptx0wgfsnraq4xi/shape_predictor_68_face_landmarks.dat.bz2?dl=1\"\n",
    "# \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "LANDMARKS_FILENAME = \"shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "\n",
    "def unpack_bz2(src_path):\n",
    "    data = bz2.BZ2File(src_path).read()\n",
    "    dst_path = src_path[:-4]\n",
    "    with open(dst_path, 'wb') as fp:\n",
    "        fp.write(data)\n",
    "    return dst_path\n",
    "  \n",
    "def image_align(img, face_landmarks, output_size=1024, transform_size=4096, enable_padding=True):\n",
    "    # Align function from FFHQ dataset pre-processing step\n",
    "    # https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n",
    "\n",
    "    lm = np.array(face_landmarks)\n",
    "    lm_chin          = lm[0  : 17]  # left-right\n",
    "    lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
    "    lm_eyebrow_right = lm[22 : 27]  # left-right\n",
    "    lm_nose          = lm[27 : 31]  # top-down\n",
    "    lm_nostrils      = lm[31 : 36]  # top-down\n",
    "    lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
    "    lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
    "    lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
    "    lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
    "\n",
    "    # Calculate auxiliary vectors.\n",
    "    eye_left     = np.mean(lm_eye_left, axis=0)\n",
    "    eye_right    = np.mean(lm_eye_right, axis=0)\n",
    "    eye_avg      = (eye_left + eye_right) * 0.5\n",
    "    eye_to_eye   = eye_right - eye_left\n",
    "    mouth_left   = lm_mouth_outer[0]\n",
    "    mouth_right  = lm_mouth_outer[6]\n",
    "    mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
    "    eye_to_mouth = mouth_avg - eye_avg\n",
    "\n",
    "    # Choose oriented crop rectangle.\n",
    "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
    "    x /= np.hypot(*x)\n",
    "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
    "    y = np.flipud(x) * [-1, 1]\n",
    "    c = eye_avg + eye_to_mouth * 0.1\n",
    "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
    "    qsize = np.hypot(*x) * 2\n",
    "\n",
    "    # Shrink.\n",
    "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
    "    if shrink > 1:\n",
    "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
    "        dst_img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
    "        quad /= shrink\n",
    "        qsize /= shrink\n",
    "    else:\n",
    "        dst_img = img.copy()\n",
    "\n",
    "    # Crop.\n",
    "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
    "    crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))),\n",
    "            int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
    "    crop = (max(crop[0] - border, 0),\n",
    "            max(crop[1] - border, 0),\n",
    "            min(crop[2] + border, dst_img.size[0]),\n",
    "            min(crop[3] + border, dst_img.size[1]))\n",
    "    if crop[2] - crop[0] < dst_img.size[0] or crop[3] - crop[1] < dst_img.size[1]:\n",
    "        dst_img = dst_img.crop(crop)\n",
    "        quad -= crop[0:2]\n",
    "\n",
    "    # Pad.\n",
    "    pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
    "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - dst_img.size[0] + border, 0), max(pad[3] - dst_img.size[1] + border, 0))\n",
    "    if enable_padding and max(pad) > border - 4:\n",
    "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
    "        dst_img = np.pad(np.float32(dst_img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
    "        h, w, _ = dst_img.shape\n",
    "        y, x, _ = np.ogrid[:h, :w, :1]\n",
    "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
    "        blur = qsize * 0.02\n",
    "        dst_img += (scipy.ndimage.gaussian_filter(dst_img, [blur, blur, 0]) - dst_img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
    "        dst_img += (np.median(dst_img, axis=(0,1)) - dst_img) * np.clip(mask, 0.0, 1.0)\n",
    "        dst_img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(dst_img), 0, 255)), 'RGB')\n",
    "        quad += pad[:2]\n",
    "\n",
    "    # Transform.\n",
    "    dst_img = dst_img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
    "    if output_size < transform_size:\n",
    "        dst_img = dst_img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
    "\n",
    "    return dst_img\n",
    "  \n",
    "class LandmarksDetector:\n",
    "    def __init__(self, predictor_model_path):\n",
    "        \"\"\"\n",
    "        :param predictor_model_path: path to shape_predictor_68_face_landmarks.dat file\n",
    "        \"\"\"\n",
    "        self.detector = dlib.get_frontal_face_detector() # cnn_face_detection_model_v1 also can be used\n",
    "        self.shape_predictor = dlib.shape_predictor(predictor_model_path)\n",
    "\n",
    "    def get_landmarks(self, img):\n",
    "        dets = self.detector(img, 1)\n",
    "\n",
    "        for detection in dets:\n",
    "            face_landmarks = [(item.x, item.y) for item in self.shape_predictor(img, detection).parts()]\n",
    "            yield face_landmarks\n",
    "            \n",
    "def convert_images_loss(images):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = tf.transpose(images, [0, 2, 3, 1])\n",
    "    drange=[-1,1]\n",
    "    scale = 255 / 2\n",
    "    images = images * scale + scale + 0.5\n",
    "    return images\n",
    "  \n",
    "def convert_images_gen(images):\n",
    "    images = tf.saturate_cast(images, tf.uint8)\n",
    "    return images\n",
    "  \n",
    "class Evaluator:\n",
    "    def __init__(self, aligned_img, loss_grad_func, latent_shape):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "        self.latent_shape = latent_shape\n",
    "        \n",
    "        aligned_img = np.asarray(aligned_img)\n",
    "        if len(aligned_img.shape) == 4:\n",
    "            self.aligned_img = aligned_img\n",
    "        elif len(aligned_img.shape) == 3:\n",
    "            self.aligned_img = np.expand_dims(aligned_img, 0)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported image shape '{}'.\".format(aligned_img.shape))\n",
    "\n",
    "        self.loss_grad_func = loss_grad_func\n",
    "        \n",
    "        self.eval_iter = 0\n",
    "\n",
    "    def loss(self, latent):\n",
    "        assert self.loss_value is None\n",
    "        latent = latent.reshape(self.latent_shape) \n",
    "        outs = self.loss_grad_func([self.aligned_img, latent])\n",
    "        self.loss_value = outs[0]\n",
    "        self.grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "        \n",
    "        # clip the gradients\n",
    "        self.grad_values = np.maximum(np.minimum(self.grad_values, 1.0), -1.0)\n",
    "\n",
    "        self.eval_iter += 1\n",
    "        print(\"eval {}, loss {}\".format(self.eval_iter, self.loss_value))\n",
    "        \n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "def move_and_show(dlatent, direction, coeffs):\n",
    "    fig,ax = plt.subplots(1, len(coeffs), figsize=(12, 10), dpi=80)\n",
    "    dlatent = dlatent.reshape(dlatent_shape)\n",
    "    \n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        new_latent_vector = dlatent.copy()\n",
    "        new_latent_vector[:8] = (dlatent + coeff*direction)[:8]\n",
    "        ax[i].imshow(gen_func([new_latent_vector])[0][0])\n",
    "        ax[i].set_title('Coeff: %0.1f' % coeff)\n",
    "    [x.axis('off') for x in ax]\n",
    "    plt.show()\n",
    "    \n",
    "def blend_and_show(dlatent1, dlatent2, coeffs):\n",
    "    fig,ax = plt.subplots(1, len(coeffs), figsize=(12, 10), dpi=80)\n",
    "    dlatent1 = dlatent1.reshape(dlatent_shape)\n",
    "    \n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        new_latent_vector = coeff * dlatent1 + (1-coeff) * dlatent2\n",
    "        ax[i].imshow(gen_func([new_latent_vector])[0][0])\n",
    "        ax[i].set_title('Coeff: %0.1f' % coeff)\n",
    "    [x.axis('off') for x in ax]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJ7pgxWi37hM",
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Generating Human Faces using StyleGAN\n",
    "\n",
    "The notebook will show how the StyleGAN method from NVIDIA introduced in paper [\"A Style-Based Generator Architecture for Generative Adversarial Networks\"](https://arxiv.org/abs/1812.04948) can be used to generate images of human faces. The official implementation of the method can be found in [this GitHub repository](https://github.com/NVlabs/stylegan). However, we will also be using some latent vectors and pieces of code from [another GitHub repository](https://github.com/Puzer/stylegan-encoder.git).\n",
    "\n",
    "## Loading the Models\n",
    "\n",
    "As a first step we will load the pretrained GAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "wR5bmGVW_cbI",
    "outputId": "0e1676c1-4d48-4b51-cdce-cae689270523"
   },
   "outputs": [],
   "source": [
    "tflib.init_tf()\n",
    "with open(\"data/model.pkl\", \"rb\") as file:\n",
    "    _, _, Gs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Models used to align faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "bk1ZeW_1_cYP",
    "outputId": "7bfb49ef-0a8a-487a-c698-79f917975bf3"
   },
   "outputs": [],
   "source": [
    "landmarks_model_path = unpack_bz2(\n",
    "    get_file(LANDMARKS_FILENAME,\n",
    "    LANDMARKS_MODEL_URL, cache_subdir='temp')\n",
    ")\n",
    "\n",
    "landmarks_detector = LandmarksDetector(landmarks_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will separate the individual parts of the model used for face generation and create functions which will allow us to apply them. The part, which maps the original latent vector to a disentangled latent vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ai-GQYzjFe7D"
   },
   "outputs": [],
   "source": [
    "tf_map_in = Gs.components.mapping.input_templates[0]\n",
    "tf_map_out = Gs.components.mapping.output_templates[0]\n",
    "map_func = K.function([tf_map_in], [tf_map_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The part, which generates images from the disentangled latent vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M21yxOrnHC-y"
   },
   "outputs": [],
   "source": [
    "tf_dlatents = Gs.components.synthesis.input_templates[0]\n",
    "tf_output = Gs.components.synthesis.output_templates[0]\n",
    "tf_loss_img_out = convert_images_loss(tf_output)\n",
    "tf_img_out = convert_images_gen(tf_loss_img_out)\n",
    "gen_func = K.function([tf_dlatents], [tf_img_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will store the the shape of the original and the disentangled latent vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_D4PSezvHbO7"
   },
   "outputs": [],
   "source": [
    "latent_shape = (1,) + K.int_shape(tf_map_in)[1:]\n",
    "dlatent_shape = (1,) + K.int_shape(tf_dlatents)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Random Face Generation\n",
    "\n",
    "Next we will generate a random face. We will start by generating a latent vector. Its elements will be drawn from the normal distribution and the shape will be according to variable ``latent_shape``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XszhQdBNHSv9"
   },
   "outputs": [],
   "source": [
    "latents = np.random.randn(*latent_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will form the disentangled latent vector by applying function ``map_func`` (defined above) to the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "96RGILlrHSmA",
    "outputId": "e7dd7939-9b7f-40b6-f2a8-420667642161"
   },
   "outputs": [],
   "source": [
    "dlatents = map_func([latents])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We can next use ``dlatents`` as an input to ``gen_func``, which will generate the image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPxqi2CtHSWT"
   },
   "outputs": [],
   "source": [
    "img = gen_func([dlatents])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Afterwards all we need to do is to visualize the image, or perhaps save it into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "xiJnyej6IPeJ",
    "outputId": "fce25713-c0b4-4476-9bb0-d5c2edbe8945"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHNP7xSik-70",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Generating more images can be tried here – we just need to generate a new latent vector every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "colab_type": "code",
    "id": "bejvsi-cIPQZ",
    "outputId": "2dd8afa1-c1ff-486b-ff1e-f2779c26158c"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(10, 10))\n",
    "\n",
    "for row in axes:\n",
    "    for ax in row:\n",
    "        latents = np.random.randn(*latent_shape)\n",
    "        dlatents = map_func([latents])[0]\n",
    "        img = gen_func([dlatents])[0][0]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Disentangled Latent Vector of an Existing Face\n",
    "\n",
    "The GAN could be used to manipulate existing faces in interesting ways. However, we would first need to know their latent vectors. Unfortunately, StyleGAN only work one way: it generates faces out of latent vectors, but not vice versa.\n",
    "\n",
    "Nevertheless, we can apply the same principle that we use to generate pre-images and adversarial examples. The neural net is differentiable and we can use optimization to find a latent vector whose matching face will be as similar to the target face as possible.\n",
    "\n",
    "### Face Similarity\n",
    "\n",
    "We will not measure face similarity in terms of pixel-wise distance, because that would not express the actual similarity well. We will instead preprocess the facial images using a neural net pretrained on ImageNet first. We will be comparing the resulting features instead of the raw pixels.\n",
    "\n",
    "Let us therefore create all the individual tensors and load the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7201UTf4M2sw"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "feature_img_size = 256\n",
    "aligned_img_size = 1024\n",
    "vgg_layer = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "J8Hoxgub_cMA",
    "outputId": "fa3b9702-05d4-4721-9876-c2ff08d5c888"
   },
   "outputs": [],
   "source": [
    "vgg16 = VGG16(include_top=False, input_shape=(feature_img_size, feature_img_size, 3))\n",
    "perceptual_model = Model(vgg16.input, vgg16.layers[vgg_layer].output)\n",
    "\n",
    "tf_img_ref = K.placeholder((1, aligned_img_size, aligned_img_size, 3))\n",
    "\n",
    "tf_out_resized = preprocess_input(tf.image.resize_images(tf_loss_img_out,\n",
    "                                  (feature_img_size, feature_img_size), method=1))\n",
    "tf_out_features = perceptual_model(tf_out_resized)\n",
    "\n",
    "tf_ref_resized = preprocess_input(tf.image.resize_images(tf_img_ref,\n",
    "                                  (feature_img_size, feature_img_size), method=1))\n",
    "tf_ref_features = perceptual_model(tf_ref_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Next we will need to define the loss function – we can use the mean square error between the features of the generated image and the original image. We can also rescale the loss to squash the numbers into a more reasonable range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTaqMA5NBMtD"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(tf_out_features, tf_ref_features) / 85000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As a further step we will define the gradient of the loss function w.r.t. the disentangled latent vector so that we can later use it to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFAPoq38zTBc"
   },
   "outputs": [],
   "source": [
    "tf_grads = K.gradients(loss, tf_dlatents)\n",
    "loss_grad_func = K.function([tf_img_ref, tf_dlatents], [loss] + tf_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Minimizing the Loss\n",
    "\n",
    "We will now load an image and minimize the loss function so as to find its matching latent vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXiWGihwOvF2"
   },
   "outputs": [],
   "source": [
    "face_img_path = \"data/starr.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "You can uncomment the following cell if you would rather upload your own image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mw-LtaC5OtYP"
   },
   "outputs": [],
   "source": [
    "# face_img_path = list(files.upload())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "_xIXheu9IPC5",
    "outputId": "dab04bc8-1fec-4c13-9975-d7d1f00dd66b"
   },
   "outputs": [],
   "source": [
    "face_img = PIL.Image.open(face_img_path)\n",
    "plt.imshow(face_img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "It is necessary to preprocess the image a bit. We will extract the key points and align the face to match the data on which the GAN was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "toHL99Yi_cPF",
    "outputId": "a1146d91-8b2c-4993-bb44-4f0ccd0cf0e3"
   },
   "outputs": [],
   "source": [
    "face_landmarks = next(landmarks_detector.get_landmarks(np.asarray(face_img)))\n",
    "aligned_img = image_align(face_img, face_landmarks, output_size=aligned_img_size)\n",
    "plt.imshow(aligned_img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will start optimizing from an all-zeros latent vector using LBFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yM5K3gY7Zdfv"
   },
   "outputs": [],
   "source": [
    "evaluator = Evaluator(aligned_img, loss_grad_func, dlatent_shape)\n",
    "dlatent = np.zeros(dlatent_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TfCiTSwzOzXb",
    "outputId": "05f6a92e-0913-4dc4-fe9e-3d7c7200958e"
   },
   "outputs": [],
   "source": [
    "dlatent, min_val, info = fmin_l_bfgs_b(evaluator.loss, dlatent.flatten(),\n",
    "     fprime=evaluator.grads, maxfun=400, disp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Generating the Image\n",
    "\n",
    "Having minimized the loss function, we gain the disentangled latent vector, which approximately matches the original image. When we use it to generate a new face, it should be similar to the original face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "xlVfFlHSO4QA",
    "outputId": "d1aa7050-febe-4432-bf25-a76ed9f1f27d"
   },
   "outputs": [],
   "source": [
    "img = gen_func([dlatent.reshape((dlatent_shape))])[0][0]\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Latent Vector Manipulation\n",
    "\n",
    "We can now further modify the latent vector of the image. In a way similar to various other types of GANs and embeddings, some arithmetic operations with the vectors make sense semantically. We can, for an instance, identify a vector, which approximately corresponds to a smile, to age, to gender, etc. Let us now load several such vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDoUCKI_SY-Q"
   },
   "outputs": [],
   "source": [
    "smile_direction = np.load('stylegan-encoder/ffhq_dataset/latent_directions/smile.npy')\n",
    "gender_direction = np.load('stylegan-encoder/ffhq_dataset/latent_directions/gender.npy')\n",
    "age_direction = np.load('stylegan-encoder/ffhq_dataset/latent_directions/age.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let us apply the smile vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "p9xqB3jk-BvO",
    "outputId": "a3ba7eef-c11f-439b-f646-b0e3132a3664"
   },
   "outputs": [],
   "source": [
    "move_and_show(dlatent.reshape((dlatent_shape)), smile_direction, [-1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The gender vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "aQP0YMzWSH8x",
    "outputId": "61bb002e-ff9f-43a6-e695-cdaae4b45829"
   },
   "outputs": [],
   "source": [
    "move_and_show(dlatent.reshape((dlatent_shape)), gender_direction, [-1.5, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The age vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "TskrBREDSHz_",
    "outputId": "9fbfc07c-4d64-427d-c255-598145b85719"
   },
   "outputs": [],
   "source": [
    "move_and_show(dlatent.reshape((dlatent_shape)), age_direction, [-2, 0, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Style Mixing\n",
    "\n",
    "Alternatively, we can mix styles from multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "KJrVD2VSTukR",
    "outputId": "a55032c0-97b0-4ded-f62c-b06c2af58cbd"
   },
   "outputs": [],
   "source": [
    "latent2 = np.random.RandomState(1855).randn(*latent_shape)\n",
    "dlatent2 = map_func([latent2])[0]\n",
    "img2 = gen_func([dlatent2])[0][0]\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "colab_type": "code",
    "id": "0w84shi5a-Ie",
    "outputId": "f777109d-4a47-4b81-cc49-9cd19da50ec2"
   },
   "outputs": [],
   "source": [
    "blend_and_show(dlatent.reshape(dlatent_shape), dlatent2, [0, 0.25, 0.5, 0.75, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Arithmetics\n",
    "\n",
    "The following cells show, how to find more semantic vectors: We generate more photos and keep track the random seeds corresponding to photos which do or do not contain the target property – e.g. photos with long and short hair. We then compute the difference between the latent vectors for both groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mab0xMWg1rq"
   },
   "outputs": [],
   "source": [
    "def find_mean_dlatent(seeds):\n",
    "    mean_dlatent = np.zeros(dlatent_shape)\n",
    "    \n",
    "    for s in seeds:\n",
    "        h_latent = np.random.RandomState(s).randn(*latent_shape)\n",
    "        h_dlatent = map_func([h_latent])[0]\n",
    "\n",
    "        mean_dlatent += h_dlatent / len(seeds)\n",
    "        \n",
    "    return mean_dlatent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HAirmKcfCdl"
   },
   "outputs": [],
   "source": [
    "female_long_hair = [517, 519, 521, 523, 525, 528, 529, 538, 539, 540, 618, 642, 655]\n",
    "female_short_hair = [537, 546, 561, 597, 599, 602, 610, 616, 627, 637, 652]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9XH7ZLdgVyr"
   },
   "outputs": [],
   "source": [
    "long_hair_dlatent = find_mean_dlatent(female_long_hair)\n",
    "short_hair_dlatent = find_mean_dlatent(female_short_hair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "3qnxccz8gV8d",
    "outputId": "131600a5-252c-429a-b723-125edfe4fc8d"
   },
   "outputs": [],
   "source": [
    "dlatent2 = dlatent.reshape(dlatent_shape) + 0.5 * (short_hair_dlatent - long_hair_dlatent)\n",
    "img2 = gen_func([dlatent2])[0][0]\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1J2K2xFT2Od"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "10_StyleGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
